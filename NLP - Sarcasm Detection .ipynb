{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 2 - Sequential Models in NLP - Sarcasm Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7SIeJ_oZMU",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1tpOCamr9aWz817atPnyXus8w5gJ3mIts\" width=500px>\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eukag7wEoPZu",
        "colab_type": "text"
      },
      "source": [
        "### Package Version:\n",
        "- tensorflow==2.2.0\n",
        "- pandas==1.0.5\n",
        "- numpy==1.18.5\n",
        "- google==2.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pp68FAQf9aMN"
      },
      "source": [
        "# Sarcasm Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEahVPtWX5ve",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "#### Acknowledgement\n",
        "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
        "\n",
        "**Required Files given in below link.**\n",
        "\n",
        "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vAk6BRUh8CqL"
      },
      "source": [
        "### Load Data (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v8-PQsV0DrAZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z6pXf7A78E2H"
      },
      "source": [
        "### Drop `article_link` from dataset (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WUNHq5zEV0n",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0h6IOxU8OdH"
      },
      "source": [
        "### Get length of each headline and add a column for that (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MLpiBRDmEV2l",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMF-wjJ2aMwm",
        "colab_type": "text"
      },
      "source": [
        "### Initialize parameter values\n",
        "- Set values for max_features, maxlen, & embedding_size\n",
        "- max_features: Number of words to take from tokenizer(most frequent words)\n",
        "- maxlen: Maximum length of each sentence to be limited to 25\n",
        "- embedding_size: size of embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jPw9gAN_EV6m",
        "colab": {}
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 25\n",
        "embedding_size = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9abSe-bM8fn9"
      },
      "source": [
        "### Apply `tensorflow.keras` Tokenizer and get indices for words (5 Marks)\n",
        "- Initialize Tokenizer object with number of words as 10000\n",
        "- Fit the tokenizer object on headline column\n",
        "- Convert the text to sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8g4l0KfF3eh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeZpwPO4bOkZ",
        "colab_type": "text"
      },
      "source": [
        "### Pad sequences (5 Marks)\n",
        "- Pad each example with a maximum length\n",
        "- Convert target column into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV0K70E5c9Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJLyKg-98rH_"
      },
      "source": [
        "### Vocab mapping\n",
        "- There is no word for 0th index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCNgtnQqdbZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRiNX58Rb3oJ",
        "colab_type": "text"
      },
      "source": [
        "### Set number of words\n",
        "- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dfwq6ou8ck2f",
        "colab": {}
      },
      "source": [
        "num_words = len(tokenizer.word_index) + 1\n",
        "print(num_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bUF1TuQa8ux0"
      },
      "source": [
        "### Load Glove Word Embeddings (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vq5AIfRtMeZh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHSzdQUcZhm",
        "colab_type": "text"
      },
      "source": [
        "### Create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elZ-T5aFGZmZ",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = './glove.6B.200d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    word = o.split(\" \")[0]\n",
        "    # print(word)\n",
        "    embd = o.split(\" \")[1:]\n",
        "    embd = np.asarray(embd, dtype='float32')\n",
        "    # print(embd)\n",
        "    embeddings[word] = embd\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7IbWuEX82Ra"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Hint: Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required. \n",
        "In the end add a final dense layer with sigmoid activation for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv168Gmc3PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoI7_8Y1cqTj",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJiPHeNoJ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s4nmqcecw3a",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NN789zNnJ5PL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}